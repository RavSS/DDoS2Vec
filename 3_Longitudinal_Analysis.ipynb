{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Longitudinal Analysis\n",
    "This notebook recreates the longitudinal analysis of a DDoS2Vec model as described in the paper.\n",
    "\n",
    "**Warning**: Unlike previous notebooks, this notebook is documented less thoroughly. This is because it is already specific to our dataset in terms of the data's time ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import collections as cll\n",
    "import datetime as dt\n",
    "import gc\n",
    "import gzip\n",
    "import itertools as it\n",
    "import multiprocessing as mp\n",
    "import multiprocessing.pool as mp_pool\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import typing as T\n",
    "\n",
    "import numpy as np\n",
    "import numpy.typing as T_np\n",
    "import joblib\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global notebook options and variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "ARTEFACTS_PATH = \"./Artefacts/\"  # Where the monthly corpora are located.\n",
    "\n",
    "SEED = 463  # Same as in `2_Baseline_Comparison.ipynb`.\n",
    "\n",
    "CLASSIFIER_NAME: T.Literal[\n",
    "    \"k-NN\",\n",
    "    \"XGBoost\",\n",
    "    \"Dummy\",\n",
    "] = \"k-NN\"\n",
    "\n",
    "# By having the training month as 6 (June), we can test a few months into the\n",
    "# past up to 1 (January) and test a few months into the future up to 12\n",
    "# (December). This is the best analysis we can do with the least amount of\n",
    "# processing/compute, instead of training on one month and testing on every\n",
    "# other month with all months getting a chance to be the training month.\n",
    "TRAINING_MONTH: int = 6\n",
    "MAX_CONCURRENT_MONTHS: T.Optional[int] = 2\n",
    "\n",
    "# We used the untuned hyperparameter configuration from the previous\n",
    "# experiments in the paper.\n",
    "SVD_COMPONENTS = 100\n",
    "NGRAM_RANGE = (1, 3)\n",
    "\n",
    "BINARISER_TRAIN_PATH = os.path.join(\n",
    "    ARTEFACTS_PATH, \"binariser_train.pickle_joblib.gz\"\n",
    ")\n",
    "TF_IDF_PATH = os.path.join(ARTEFACTS_PATH, \"tf_idf.pickle_joblib.gz\")\n",
    "SVD_PATH = os.path.join(ARTEFACTS_PATH, \"svd.pickle_joblib.gz\")\n",
    "CLASSIFIER_PATH = os.path.join(\n",
    "    ARTEFACTS_PATH, f\"{CLASSIFIER_NAME}.pickle_joblib.gz\"\n",
    ")\n",
    "\n",
    "binariser_train: MultiLabelBinarizer\n",
    "tf_idf: TfidfVectorizer\n",
    "svd: TruncatedSVD"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# This is a hack to make the default multiprocessing pool not spawn daemon\n",
    "# processes; hence, we can then use pools within child processes made by a\n",
    "# pool. Will cause a lot of zombie processes or fail to exit cleanly if\n",
    "# something ancestral crashes, as they're no longer marked daemonic.\n",
    "class NoDaemonProcess(mp.Process):\n",
    "    @property\n",
    "    def daemon(self):\n",
    "        return False\n",
    "\n",
    "    @daemon.setter\n",
    "    def daemon(self, _):\n",
    "        pass\n",
    "\n",
    "\n",
    "class CustomPool(mp_pool.Pool):  # See above.\n",
    "    def Process(self, *args, **kwargs):\n",
    "        proc = super().Process(*args, **kwargs)  # type: ignore\n",
    "        proc.__class__ = NoDaemonProcess\n",
    "        return proc\n",
    "\n",
    "\n",
    "LOG_LOCK = mp.Lock()\n",
    "\n",
    "\n",
    "def log(*args, **kwargs):\n",
    "    with LOG_LOCK:\n",
    "        print(\n",
    "            f\"[{dt.datetime.now().strftime('%d-%m %I:%M:%S %p')}]:\",\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "\n",
    "def load_pickle(path: str, compressed=True) -> T.Any:\n",
    "    try:\n",
    "        gc.disable()\n",
    "        if compressed:\n",
    "            assert path.endswith(\".gz\")\n",
    "            with gzip.open(path, \"rb\") as file:\n",
    "                return pickle.load(file)\n",
    "        else:\n",
    "            with open(path, \"rb\") as file:\n",
    "                return pickle.load(file)\n",
    "    except Exception as exception:\n",
    "        raise exception from None\n",
    "    finally:\n",
    "        gc.enable()\n",
    "\n",
    "\n",
    "def save_pickle(object: T.Any, path: str, overwrite=True, compress=True):\n",
    "    if overwrite or not os.path.isfile(path):\n",
    "        if compress:\n",
    "            assert path.endswith(\".gz\")\n",
    "            with gzip.open(path, \"wb\", compresslevel=6) as file:\n",
    "                pickle.dump(object, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        else:\n",
    "            with open(path, \"wb\") as file:\n",
    "                pickle.dump(object, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def build_classifier():\n",
    "    log(f'Using classifier \"{CLASSIFIER_NAME}\".')\n",
    "\n",
    "    if CLASSIFIER_NAME == \"k-NN\":\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "        return KNeighborsClassifier(\n",
    "            n_neighbors=10, n_jobs=-1, weights=\"distance\"\n",
    "        )\n",
    "    elif CLASSIFIER_NAME == \"XGBoost\":\n",
    "        from xgboost import XGBClassifier\n",
    "\n",
    "        return XGBClassifier(tree_method=\"hist\", n_jobs=-1, random_state=SEED)\n",
    "    elif CLASSIFIER_NAME == \"Dummy\":\n",
    "        from sklearn.dummy import DummyClassifier\n",
    "\n",
    "        return DummyClassifier(random_state=SEED)\n",
    "\n",
    "    raise ValueError(f\"Unknown classifier: {CLASSIFIER_NAME}\")\n",
    "\n",
    "\n",
    "# Using a lambda function here causes a pickling error when trying to save the\n",
    "# TF-IDF vectoriser. That is likely a Jupyter Notebook issue.\n",
    "def return_argument(argument: T.Any) -> T.Any:\n",
    "    return argument"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the corpus, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def load_corpus(\n",
    "    month: T.Union[int, str], binariser: T.Optional[MultiLabelBinarizer] = None\n",
    "):\n",
    "    corpus: T.Dict[str, T.List[str]] = load_pickle(\n",
    "        os.path.join(ARTEFACTS_PATH, f\"{month}.standard.data.pickle.gz\")\n",
    "    )\n",
    "    labels: T.Dict[str, T.Dict[T.Optional[str], int]] = load_pickle(\n",
    "        os.path.join(ARTEFACTS_PATH, f\"{month}.standard.labels.pickle.gz\")\n",
    "    )\n",
    "\n",
    "    x: T.List[T.Tuple[str, ...]] = []\n",
    "    y_tags: T.List[str] = []\n",
    "\n",
    "    while corpus:\n",
    "        tag, words = corpus.popitem()\n",
    "        x.append(tuple(words))\n",
    "        y_tags.append(tag)\n",
    "\n",
    "    label_generator = (\n",
    "        (label if label is not None else \"-\" for label in labels[tag])\n",
    "        for tag in y_tags\n",
    "    )\n",
    "    if binariser is not None:\n",
    "        binariser_classes = frozenset(binariser.classes_).union((None,))\n",
    "        for tag_labels in labels.values():\n",
    "            for tag_label in frozenset(tag_labels):\n",
    "                if tag_label not in binariser_classes:\n",
    "                    if None in tag_labels:\n",
    "                        tag_labels[None] += tag_labels[tag_label]\n",
    "                    else:\n",
    "                        tag_labels[None] = tag_labels[tag_label]\n",
    "                    del tag_labels[tag_label]\n",
    "        y: T_np.NDArray[T.Any] = binariser.transform(\n",
    "            label_generator\n",
    "        )  # type: ignore\n",
    "    else:\n",
    "        binariser = MultiLabelBinarizer(sparse_output=False)\n",
    "        y: T_np.NDArray[T.Any] = binariser.fit_transform(\n",
    "            label_generator\n",
    "        )  # type: ignore\n",
    "\n",
    "    return x, y, y_tags, binariser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "log(\"Loading corpus...\")\n",
    "x_train, y_train, y_tags_train, binariser_train = load_corpus(TRAINING_MONTH)\n",
    "joblib.dump(\n",
    "    binariser_train,\n",
    "    BINARISER_TRAIN_PATH,\n",
    "    compress=6,\n",
    "    protocol=pickle.HIGHEST_PROTOCOL,\n",
    ")\n",
    "log(\"Loaded corpus.\")\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model on the chosen month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "tf_idf = TfidfVectorizer(\n",
    "    ngram_range=NGRAM_RANGE,\n",
    "    lowercase=False,\n",
    "    tokenizer=return_argument,\n",
    "    preprocessor=return_argument,\n",
    "    token_pattern=None,  # type: ignore\n",
    "    use_idf=True,\n",
    "    smooth_idf=True,\n",
    "    sublinear_tf=True,\n",
    ")\n",
    "\n",
    "log(\"Fitting TF-IDF vectoriser...\")\n",
    "tf_idf_matrix_train = tf_idf.fit_transform(x_train)\n",
    "log(\"Fitted TF-IDF vectoriser.\")\n",
    "joblib.dump(tf_idf, TF_IDF_PATH, compress=6, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "log(\"Saved TF-IDF vectoriser.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The low dimensional SVD is just for visualization purposes.\n",
    "The below code cell can be removed if time is an issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "low_dimensional_svd = TruncatedSVD(\n",
    "    n_components=2,\n",
    "    algorithm=\"arpack\",\n",
    "    random_state=SEED,\n",
    ")\n",
    "\n",
    "log(\"Fitting low dimensional SVD...\")\n",
    "low_dimensional_decomposition = low_dimensional_svd.fit_transform(\n",
    "    tf_idf_matrix_train\n",
    ")\n",
    "log(\"Fitted low dimensional SVD.\")\n",
    "joblib.dump(\n",
    "    low_dimensional_decomposition,\n",
    "    os.path.join(\n",
    "        ARTEFACTS_PATH, \"low_dimensional_decomposition.pickle_joblib.gz\"\n",
    "    ),\n",
    "    compress=6,\n",
    ")\n",
    "log(\"Saved low-dimensional transformed TF-IDF matrix.\")\n",
    "del low_dimensional_svd, low_dimensional_decomposition  # Save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(\n",
    "    n_components=SVD_COMPONENTS,\n",
    "    algorithm=\"arpack\",\n",
    "    random_state=SEED,\n",
    ")\n",
    "\n",
    "log(\"Fitting SVD...\")\n",
    "decomposition_train = svd.fit_transform(tf_idf_matrix_train)\n",
    "log(\"Fitted SVD.\")\n",
    "joblib.dump(svd, SVD_PATH, compress=6, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "joblib.dump(\n",
    "    decomposition_train,\n",
    "    os.path.join(ARTEFACTS_PATH, \"decomposition_train.pickle_joblib.gz\"),\n",
    "    compress=8,\n",
    "    protocol=pickle.HIGHEST_PROTOCOL,\n",
    ")\n",
    "log(\"Saved SVD and transformed TF-IDF matrix.\")\n",
    "\n",
    "classifier = build_classifier()\n",
    "log(f\"Fitting {CLASSIFIER_NAME}...\")\n",
    "classifier = classifier.fit(decomposition_train, y_train)\n",
    "log(f\"Fitted {CLASSIFIER_NAME}.\")\n",
    "joblib.dump(\n",
    "    classifier, CLASSIFIER_PATH, compress=6, protocol=pickle.HIGHEST_PROTOCOL\n",
    ")\n",
    "log(f\"Saved {CLASSIFIER_NAME}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training of the model is done at this point. \n",
    "\n",
    "The below cell just loads the persisted model (if not already in the namespace)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    binariser_train  # type: ignore\n",
    "except NameError:\n",
    "    binariser_train: MultiLabelBinarizer = joblib.load(BINARISER_TRAIN_PATH)\n",
    "\n",
    "try:\n",
    "    tf_idf  # type: ignore\n",
    "except NameError:\n",
    "    tf_idf: TfidfVectorizer = joblib.load(TF_IDF_PATH)\n",
    "\n",
    "try:\n",
    "    svd  # type: ignore\n",
    "except NameError:\n",
    "    svd: TruncatedSVD = joblib.load(SVD_PATH)\n",
    "\n",
    "try:\n",
    "    classifier  # type: ignore\n",
    "except NameError:\n",
    "    classifier = joblib.load(CLASSIFIER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "def test(month: T.Union[int, str]):\n",
    "    scores_file = os.path.join(ARTEFACTS_PATH, f\"{month}.scores.pickle.gz\")\n",
    "    if os.path.isfile(scores_file):\n",
    "        log(f\"[{month}] Scores already computed.\")\n",
    "        return load_pickle(scores_file)\n",
    "\n",
    "    log(f\"[{month}] Loading corpus...\")\n",
    "    x_test, y_test, _, _ = load_corpus(month, binariser_train)\n",
    "\n",
    "    start_time = dt.datetime.now(tz=dt.timezone.utc)\n",
    "\n",
    "    log(f\"[{month}] Computing TF-IDF...\")\n",
    "    tf_idf_matrix_test = tf_idf.transform(x_test)\n",
    "\n",
    "    log(f\"[{month}] Computing the SVD of the TF-IDF matrix...\")\n",
    "    decomposition_test = svd.transform(tf_idf_matrix_test)\n",
    "\n",
    "    log(f\"[{month}] Predicting...\")\n",
    "    y_predictions = classifier.predict(decomposition_test)\n",
    "\n",
    "    end_time = dt.datetime.now(tz=dt.timezone.utc)\n",
    "\n",
    "    log(f\"[{month}] Scoring individual class labels...\")\n",
    "\n",
    "    individual_scores: T.Dict[str, T.Dict[str, float]] = {}\n",
    "    for index, scores in enumerate(\n",
    "        zip(\n",
    "            *np.asarray(\n",
    "                metrics.precision_recall_fscore_support(\n",
    "                    y_test, y_predictions, average=None, zero_division=0\n",
    "                )\n",
    "            ),\n",
    "            np.asarray(\n",
    "                metrics.jaccard_score(\n",
    "                    y_test, y_predictions, average=None, zero_division=0\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "    ):\n",
    "        individual_scores[binariser_train.classes_[index]] = {\n",
    "            \"Precision\": scores[0],\n",
    "            \"Recall\": scores[1],\n",
    "            \"F1 Score\": scores[2],\n",
    "            \"Support\": scores[3],\n",
    "            \"Jaccard Score\": scores[4],\n",
    "        }\n",
    "\n",
    "    for index, confusion_matrix in enumerate(\n",
    "        metrics.multilabel_confusion_matrix(y_test, y_predictions)\n",
    "    ):\n",
    "        individual_scores[binariser_train.classes_[index]].update(\n",
    "            {\n",
    "                \"False Negatives\": confusion_matrix[1][0],\n",
    "                \"True Negatives\": confusion_matrix[0][0],\n",
    "                \"False Positives\": confusion_matrix[0][1],\n",
    "                \"True Positives\": confusion_matrix[1][1],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    log(f\"[{month}] Scoring over samples...\")\n",
    "\n",
    "    precision, recall, f1_score, _ = metrics.precision_recall_fscore_support(\n",
    "        y_test, y_predictions, average=\"samples\", zero_division=0\n",
    "    )\n",
    "\n",
    "    scores = {\n",
    "        \"Start Time\": start_time,\n",
    "        \"End Time\": end_time,\n",
    "        \"Classes\": individual_scores,\n",
    "        \"Average (Samples)\": {\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1 Score\": f1_score,\n",
    "            \"Jaccard Score\": metrics.jaccard_score(\n",
    "                y_test, y_predictions, average=\"samples\", zero_division=0\n",
    "            ),\n",
    "            \"Exact Match Ratio\": metrics.accuracy_score(y_test, y_predictions),\n",
    "            \"Hamming Loss\": metrics.hamming_loss(y_test, y_predictions),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    save_pickle(scores, scores_file)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "scores_file_path = os.path.join(ARTEFACTS_PATH, \"scores.pickle.gz\")\n",
    "if not os.path.isfile(scores_file_path):\n",
    "    if MAX_CONCURRENT_MONTHS and MAX_CONCURRENT_MONTHS > 1:\n",
    "        with CustomPool(MAX_CONCURRENT_MONTHS, maxtasksperchild=1) as pool:\n",
    "            scores = pool.map(test, range(1, 13))\n",
    "    else:\n",
    "        scores = list(map(test, range(1, 13)))\n",
    "\n",
    "    save_pickle(scores, scores_file_path)\n",
    "else:\n",
    "    log(\"Scores already exist. Loading...\")\n",
    "    scores = load_pickle(scores_file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisations, etc.\n",
    "\n",
    "**Warning**: The following code is extremely messy and not well documented. \n",
    "It is not recommended to use this code for anything other than the visualisations in the paper.\n",
    "We provide this purely as an example of what you could do with a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "\n",
    "set_matplotlib_formats(\"svg\", \"pdf\")\n",
    "\n",
    "\n",
    "# An example of recalculating some metrics in case the testing set's individual\n",
    "# sample predictions are lost.\n",
    "def macro_and_micro(month_scores):\n",
    "    precisions, recalls, f1_scores = [], [], []\n",
    "    true_positives, false_positives, false_negatives = 0, 0, 0\n",
    "    for class_scores in month_scores[\"Classes\"].values():\n",
    "        precisions.append(class_scores[\"Precision\"])\n",
    "        recalls.append(class_scores[\"Recall\"])\n",
    "        f1_scores.append(class_scores[\"F1 Score\"])\n",
    "        true_positives += int(class_scores[\"True Positives\"])\n",
    "        false_positives += int(class_scores[\"False Positives\"])\n",
    "        false_negatives += int(class_scores[\"False Negatives\"])\n",
    "\n",
    "    macro_precision = float(np.mean(precisions))\n",
    "    macro_recall = float(np.mean(recalls))\n",
    "    macro_f1_score = float(np.mean(f1_scores))\n",
    "    micro_precision = true_positives / (true_positives + false_positives)\n",
    "    micro_recall = true_positives / (true_positives + false_negatives)\n",
    "    micro_f1_score = (\n",
    "        2\n",
    "        * true_positives\n",
    "        / (2 * true_positives + false_positives + false_negatives)\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        (\n",
    "            (macro_precision + micro_precision) / 2,\n",
    "            (macro_recall + micro_recall) / 2,\n",
    "            (macro_f1_score + micro_f1_score) / 2,\n",
    "        ),\n",
    "        (macro_precision, macro_recall, macro_f1_score),\n",
    "        (micro_precision, micro_recall, micro_f1_score),\n",
    "    )\n",
    "\n",
    "\n",
    "metric = \"Exact Match Ratio\"\n",
    "\n",
    "x_axis = list(range(1, 13))\n",
    "y_axis_one_label = \"Exact Match Ratio (Subset Accuracy)\"\n",
    "y_axis_one = [score[\"Average (Samples)\"][metric] for score in scores]\n",
    "\n",
    "# Some months have next to no support (number of samples) for a filtering\n",
    "# rule, so because they are extremely insignificant, they are hard to predict.\n",
    "y_axis_two_label = (\n",
    "    \"Macro-Averaged F1 Scores of Rules with \"\n",
    "    \"at least 1% of the Month's Majority Label Support\"\n",
    ")\n",
    "y_axis_two = [\n",
    "    np.mean(\n",
    "        [\n",
    "            label[\"F1 Score\"]\n",
    "            for label in score[\"Classes\"].values()\n",
    "            if label[\"Support\"]\n",
    "            >= (\n",
    "                max(\n",
    "                    majority[\"Support\"]\n",
    "                    for majority in score[\"Classes\"].values()\n",
    "                )\n",
    "                * 0.01  # Must have 1% of the majority label's support.\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    for score in scores\n",
    "]\n",
    "\n",
    "top_classes = list(\n",
    "    class_label\n",
    "    for class_label, _ in sorted(\n",
    "        scores[TRAINING_MONTH - 1][\"Classes\"].items(),\n",
    "        key=lambda x: x[1][\"Support\"],\n",
    "        reverse=True,\n",
    "    )[:15]\n",
    ")\n",
    "metric = \"Precision\"\n",
    "y_axis_three_label = f\"Macro-Averaged {metric} of Top 15 Rules by Support\"\n",
    "y_axis_three = [\n",
    "    np.mean([score[\"Classes\"][top_class][metric] for top_class in top_classes])\n",
    "    for score in scores\n",
    "]\n",
    "\n",
    "y_axis_bar_colours = [\"g\" for _ in range(len(y_axis_three))]\n",
    "y_axis_bar_colours[TRAINING_MONTH - 1] = \"purple\"\n",
    "\n",
    "plt.gca().set_ylim((0.0, 1.0))\n",
    "plt.gca().set_xlabel(\"Month of 2019\")\n",
    "plt.gca().set_ylabel(\"Score/Metric Value\")\n",
    "plt.gca().set_xticks(x_axis)\n",
    "plt.gca().set_yticks(np.arange(0.0, 1.1, 0.1))\n",
    "plt.gcf().set_size_inches(10, 4)\n",
    "plt.plot(x_axis, y_axis_one, \"-o\", label=y_axis_one_label, color=\"orange\")\n",
    "plt.bar(x_axis, y_axis_two, color=y_axis_bar_colours, label=y_axis_two_label)\n",
    "plt.plot(x_axis, y_axis_three, \"-o\", label=y_axis_three_label, color=\"cyan\")\n",
    "plt.legend(loc=\"lower center\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    os.path.join(ARTEFACTS_PATH, \"figure.longitudinal_analysis_overview.pdf\")\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabularies_file_path = os.path.join(\n",
    "    ARTEFACTS_PATH, \"month_vocabularies.pickle.gz\"\n",
    ")\n",
    "\n",
    "\n",
    "def load_vocabulary(month: T.Union[int, str]):\n",
    "    vocabulary: T.DefaultDict[str, int] = cll.defaultdict(int)\n",
    "    log(f\"Loading corpus for month {month}...\")\n",
    "    corpus: T.Dict[str, T.List[str]] = load_pickle(\n",
    "        os.path.join(ARTEFACTS_PATH, f\"{month}.standard.data.pickle.gz\")\n",
    "    )\n",
    "    log(f\"Calculating vocabulary for month {month}...\")\n",
    "    while corpus:\n",
    "        for word in corpus.popitem()[1]:\n",
    "            vocabulary[word] += 1\n",
    "\n",
    "    log(f\"Vocabulary for month {month} calculated.\")\n",
    "    return dict(vocabulary)\n",
    "\n",
    "\n",
    "if not os.path.isfile(vocabularies_file_path):\n",
    "    log(\"Calculating vocabulary counts...\")\n",
    "    with mp.Pool(min(os.cpu_count() or 8, 3)) as pool:\n",
    "        vocabularies = pool.map(load_vocabulary, range(1, 13))\n",
    "    save_pickle(vocabularies, vocabularies_file_path)\n",
    "else:\n",
    "    log(\"Vocabulary counts already exist. Loading...\")\n",
    "    vocabularies = load_pickle(vocabularies_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_vocabulary = vocabularies[TRAINING_MONTH - 1]\n",
    "\n",
    "past_vocabularies = list(reversed(vocabularies[: TRAINING_MONTH - 1]))\n",
    "future_vocabularies = list(reversed(vocabularies[TRAINING_MONTH:]))\n",
    "\n",
    "testing_vocabulary_counts: T.DefaultDict[str, T.List[int]] = cll.defaultdict(\n",
    "    list\n",
    ")\n",
    "for index, vocabulary in enumerate(vocabularies, start=1):\n",
    "    if index != TRAINING_MONTH:\n",
    "        for word, count in vocabulary.items():\n",
    "            testing_vocabulary_counts[word].append(count)\n",
    "\n",
    "testing_vocabulary = {\n",
    "    word: float(np.mean(counts))\n",
    "    for word, counts in testing_vocabulary_counts.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import zeta\n",
    "\n",
    "sorted_word_counts = list(\n",
    "    sorted(training_vocabulary.items(), key=lambda item: item[1], reverse=True)\n",
    ")\n",
    "\n",
    "x = list(range(len(sorted_word_counts)))\n",
    "y = [count for _, count in sorted_word_counts]\n",
    "\n",
    "sorted_word_counts = list(\n",
    "    sorted(testing_vocabulary.items(), key=lambda item: item[1], reverse=True)\n",
    ")\n",
    "\n",
    "test_x = list(range(len(sorted_word_counts)))\n",
    "test_y = [count for _, count in sorted_word_counts]\n",
    "\n",
    "a = 1.78\n",
    "zipf_y = sum(y) * (np.array(x) ** -a) / zeta(a)\n",
    "\n",
    "plt.clf()\n",
    "ax = plt.gca()\n",
    "ax.loglog(\n",
    "    x,\n",
    "    y,\n",
    "    label=\"Word Frequency Distribution of Training Corpus Vocabulary\",\n",
    "    color=\"blue\",\n",
    ")\n",
    "ax.loglog(\n",
    "    test_x,\n",
    "    test_y,\n",
    "    label=\"Word Frequency Distribution of \"\n",
    "    \"All Testing Corpus Vocabularies (Mean)\",\n",
    "    color=\"green\",\n",
    ")\n",
    "ax.loglog(\n",
    "    x,\n",
    "    zipf_y,\n",
    "    label=f\"Zipf Distribution (a = {a:.2f})\",\n",
    "    color=\"red\",\n",
    ")\n",
    "plt.xlim(1, len(x) * 7.75)\n",
    "plt.xlabel(\"Word Rank (log)\")\n",
    "plt.ylabel(\"Word Frequency/Occurrence (log)\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.gcf().set_size_inches(10, 4)\n",
    "plt.savefig(os.path.join(ARTEFACTS_PATH, \"figure.word_frequencies.pdf\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_of_word_count(\n",
    "    internal_vocabulary: T.Dict[str, int],\n",
    "    external_vocabulary: T.Dict[str, int],\n",
    "):\n",
    "    return {\n",
    "        word: count\n",
    "        for word, count in external_vocabulary.items()\n",
    "        if word not in internal_vocabulary\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Month, Total Words, Total Count, OOV Words, OOV Count\")\n",
    "for month, vocabulary in enumerate(vocabularies, start=1):\n",
    "    oov_words = out_of_word_count(training_vocabulary, vocabulary)\n",
    "    print(\n",
    "        month,\n",
    "        len(vocabulary),\n",
    "        sum(vocabulary.values()),\n",
    "        len(oov_words),\n",
    "        sum(oov_words.values()),\n",
    "        sep=\", \",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(vocabulary_1, vocabulary_2):\n",
    "    intersection = len(\n",
    "        list(frozenset(vocabulary_1).intersection(vocabulary_2))\n",
    "    )\n",
    "    union = (len(vocabulary_1) + len(vocabulary_2)) - intersection\n",
    "    return float(intersection) / union\n",
    "\n",
    "\n",
    "jaccard_similarities = [\n",
    "    jaccard_similarity(*vocabulary_pair)\n",
    "    for vocabulary_pair in it.product(vocabularies, vocabularies)\n",
    "]\n",
    "\n",
    "print(\n",
    "    f\"Jaccard similarity mean: {np.mean(jaccard_similarities):.3f} \"\n",
    "    f\"+- {np.std(jaccard_similarities):.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(\n",
    "    month: T.Union[str, int]\n",
    ") -> T.Dict[str, T.Dict[T.Optional[str], int]]:\n",
    "    return load_pickle(\n",
    "        os.path.join(ARTEFACTS_PATH, f\"{month}.standard.labels.pickle.gz\")\n",
    "    )\n",
    "\n",
    "\n",
    "month_labels = [get_labels(month) for month in range(1, 13)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_score = scores[TRAINING_MONTH - 1]\n",
    "past_scores = list(reversed(scores[: TRAINING_MONTH - 1]))\n",
    "future_scores = scores[TRAINING_MONTH:]\n",
    "\n",
    "\n",
    "def count_predictions(scores):\n",
    "    correct_predictions: T.List[T.List[int]] = []\n",
    "    incorrect_predictions: T.List[T.List[int]] = []\n",
    "    for score in scores:\n",
    "        correct = []\n",
    "        incorrect = []\n",
    "        for class_score in score[\"Classes\"].values():\n",
    "            correct.append(\n",
    "                int(class_score[\"True Positives\"])\n",
    "                + int(class_score[\"True Negatives\"])\n",
    "            )\n",
    "            incorrect.append(\n",
    "                int(class_score[\"False Positives\"])\n",
    "                + int(class_score[\"False Negatives\"])\n",
    "            )\n",
    "        correct_predictions.append(correct)\n",
    "        incorrect_predictions.append(incorrect)\n",
    "\n",
    "    return correct_predictions, incorrect_predictions\n",
    "\n",
    "\n",
    "training_score_correct, training_score_incorrect = count_predictions(\n",
    "    [training_score]\n",
    ")\n",
    "training_score_correct, training_score_incorrect = (\n",
    "    training_score_correct[0],\n",
    "    training_score_incorrect[0],\n",
    ")\n",
    "\n",
    "past_scores_correct, past_scores_incorrect = count_predictions(past_scores)\n",
    "future_scores_correct, future_scores_incorrect = count_predictions(\n",
    "    future_scores\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Month Distance from Training Month\")\n",
    "plt.ylabel(\"Number of Correct and Incorrect Predictions (log)\")\n",
    "\n",
    "plt.plot(\n",
    "    list(range(1, len(past_scores_correct) + 1)),\n",
    "    [sum(correct) for correct in past_scores_correct],\n",
    "    \"-o\",\n",
    "    label=\"Correct Past Predictions\",\n",
    "    color=\"b\",\n",
    ")\n",
    "plt.plot(\n",
    "    list(range(1, len(future_scores_correct) + 1)),\n",
    "    [sum(correct) for correct in future_scores_correct],\n",
    "    \"-o\",\n",
    "    label=\"Correct Future Predictions\",\n",
    "    color=\"g\",\n",
    ")\n",
    "plt.plot(\n",
    "    list(range(1, len(past_scores_incorrect) + 1)),\n",
    "    [sum(incorrect) for incorrect in past_scores_incorrect],\n",
    "    \"-o\",\n",
    "    label=\"Incorrect Past Predictions\",\n",
    "    color=\"r\",\n",
    ")\n",
    "plt.plot(\n",
    "    list(range(1, len(future_scores_incorrect) + 1)),\n",
    "    [sum(incorrect) for incorrect in future_scores_incorrect],\n",
    "    \"-o\",\n",
    "    label=\"Incorrect Future Predictions\",\n",
    "    color=\"orange\",\n",
    ")\n",
    "\n",
    "plt.legend(loc=\"center\")\n",
    "plt.tight_layout()\n",
    "plt.gcf().set_size_inches(5.5, 4)\n",
    "plt.savefig(\n",
    "    os.path.join(\n",
    "        ARTEFACTS_PATH, \"figure.longitudinal_analysis_predictions.pdf\"\n",
    "    )\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FuncFormatter, MultipleLocator\n",
    "\n",
    "x_time = list(sorted(len(labels) for labels in month_labels))\n",
    "y_time = [\n",
    "    score[\"End Time\"].timestamp() - score[\"Start Time\"].timestamp()\n",
    "    for score, _ in sorted(zip(scores, month_labels), key=lambda x: len(x[1]))\n",
    "]\n",
    "month_order = [\n",
    "    month\n",
    "    for month, _ in sorted(\n",
    "        enumerate(month_labels, start=1), key=lambda x: len(x[1])\n",
    "    )\n",
    "]\n",
    "\n",
    "plt.clf()\n",
    "plt.gca().yaxis.set_major_formatter(\n",
    "    FuncFormatter(lambda x, _: str(dt.timedelta(seconds=x))[:-3])\n",
    ")\n",
    "plt.gca().yaxis.set_major_locator(MultipleLocator(base=3600 // 4))\n",
    "plt.gca().xaxis.set_major_formatter(\n",
    "    FuncFormatter(lambda x, _: f\"{x / 1_000_000}M\")\n",
    ")\n",
    "plt.xlabel(\"Number of Documents Representing Potential Victims (millions)\")\n",
    "plt.ylabel(\"Time Taken to Test Month (H:MM)\")\n",
    "plt.gca().set_xticks(list(range(0, 10_000_000, 500_000)))\n",
    "plt.xlim(2_500_000, 6_000_000)\n",
    "for month, x, y in sorted(\n",
    "    zip(month_order, x_time, y_time), key=lambda x: x[0]\n",
    "):\n",
    "    plt.scatter(x, y, label=str(month))\n",
    "z = np.polyfit(x_time, y_time, 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(x_time, p(x_time), \"--\", color=\"black\", label=\"Trend Line\")\n",
    "plt.legend(title=\"Months\", ncol=2, loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.gcf().set_size_inches(5.5, 4)\n",
    "plt.savefig(\n",
    "    os.path.join(ARTEFACTS_PATH, \"figure.longitudinal_analysis_time.pdf\")\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Time taken for testing:\",\n",
    "    (scores[-1][\"End Time\"] - scores[0][\"Start Time\"]) / MAX_CONCURRENT_MONTHS,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
