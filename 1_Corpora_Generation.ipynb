{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpora Generation\n",
    "\n",
    "This notebook recreates all the necessary corpora for the experiments in the paper.\n",
    "\n",
    "The flow reader however needs to be implemented to suit your flow record dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import collections as cll\n",
    "import dataclasses\n",
    "import datetime as dt\n",
    "from ipaddress import (\n",
    "    IPv4Address,\n",
    "    IPv4Network,\n",
    "    IPv6Address,\n",
    "    IPv6Network,\n",
    "    ip_address,\n",
    "    ip_network,\n",
    ")\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import pickle\n",
    "import gc\n",
    "import gzip\n",
    "import typing as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Used for locking the `log` function.\n",
    "LOG_LOCK = mp.Lock()\n",
    "\n",
    "# This is where all corpora is stored. Other notebooks will expect to find\n",
    "# corpora here.\n",
    "ARTEFACTS_PATH = \"./Artefacts/\"\n",
    "\n",
    "# The month used for comparison between the various approaches. This is the\n",
    "# exact month we used in the paper.\n",
    "COMPARISON_TRAINING_DATES = (\n",
    "    (\n",
    "        dt.datetime(2019, 11, 1, tzinfo=dt.timezone.utc),\n",
    "        dt.datetime(2019, 12, 1, tzinfo=dt.timezone.utc),\n",
    "        \"month\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Notebook options.\n",
    "FORCE_OVERWRITE = True\n",
    "PACKET_SIZE_INTERVAL = 100  # The default we used in the paper.\n",
    "\n",
    "\n",
    "def log(message: str):\n",
    "    with LOG_LOCK:\n",
    "        print(\n",
    "            f\"[{dt.datetime.now().strftime('%d-%m %I:%M:%S %p')}]:\",\n",
    "            message,\n",
    "            flush=True,\n",
    "        )\n",
    "\n",
    "\n",
    "# Your flow reader implementation must return this for each flow record\n",
    "# observed. Not all fields are necessary, but the more you provide, the more\n",
    "# you can experiment with flow corpus generation.\n",
    "@dataclasses.dataclass(frozen=True, repr=True, order=True)\n",
    "class Flow:\n",
    "    timestamp: int  # Milliseconds.\n",
    "    protocol: int\n",
    "    source_address: T.Union[IPv4Address, IPv6Address]\n",
    "    source_port: int\n",
    "    destination_address: T.Union[IPv4Address, IPv6Address]\n",
    "    destination_port: int\n",
    "    bytes: int\n",
    "    packets: int\n",
    "    rule: T.Optional[str] = None  # This is specific to the IXP Scrubber rules.\n",
    "    rule_confidence: T.Optional[float] = None\n",
    "    source_ASN: T.Optional[int] = None\n",
    "    source_prefix: T.Optional[T.Union[IPv4Network, IPv6Network]] = None\n",
    "    source_country: T.Optional[str] = None\n",
    "    source_region: T.Optional[str] = None\n",
    "    destination_ASN: T.Optional[int] = None\n",
    "    destination_prefix: T.Optional[T.Union[IPv4Network, IPv6Network]] = None\n",
    "    destination_country: T.Optional[str] = None\n",
    "    destination_region: T.Optional[str] = None\n",
    "\n",
    "\n",
    "def datetime_intervals(\n",
    "    start: dt.datetime, end: dt.datetime, interval: dt.timedelta\n",
    "):\n",
    "    interval_start = start\n",
    "    while interval_start < end:\n",
    "        interval_end = min(interval_start + interval, end)\n",
    "        yield (interval_start, interval_end)\n",
    "        interval_start = interval_end\n",
    "\n",
    "\n",
    "def load_pickle(path: str, compressed=True) -> T.Any:\n",
    "    try:\n",
    "        gc.disable()\n",
    "        if compressed:\n",
    "            assert path.endswith(\".gz\")\n",
    "            with gzip.open(path, \"rb\") as file:\n",
    "                return pickle.load(file)\n",
    "        else:\n",
    "            with open(path, \"rb\") as file:\n",
    "                return pickle.load(file)\n",
    "    except Exception as exception:\n",
    "        raise exception from None\n",
    "    finally:\n",
    "        gc.enable()\n",
    "\n",
    "\n",
    "def save_pickle(\n",
    "    object: T.Any, path: str, overwrite=FORCE_OVERWRITE, compress=True\n",
    "):\n",
    "    if overwrite or not os.path.isfile(path):\n",
    "        if compress:\n",
    "            assert path.endswith(\".gz\")\n",
    "            with gzip.open(path, \"wb\", compresslevel=6) as file:\n",
    "                pickle.dump(object, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        else:\n",
    "            with open(path, \"wb\") as file:\n",
    "                pickle.dump(object, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: This is for you to implement.\n",
    "def flow_reader(start: dt.datetime, end: dt.datetime) -> T.Iterable[Flow]:\n",
    "    raise NotImplementedError(\"You must implement this for your dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "class Service:\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        packet_size_interval: int = 0,  # Negative values disable intervals.\n",
    "        packet_size_bins: T.Iterable[\n",
    "            T.Tuple[T.Optional[int], T.Optional[int]]\n",
    "        ] = (),\n",
    "        source_ports: T.Iterable[T.Union[int, T.Tuple[int, int]]] = (),\n",
    "        destination_ports: T.Iterable[T.Union[int, T.Tuple[int, int]]] = (),\n",
    "        generic=False,\n",
    "    ):\n",
    "        \"\"\"Ranges (both ports and bins) are inclusive.\"\"\"\n",
    "\n",
    "        self.name = name\n",
    "        self.packet_size_interval = packet_size_interval\n",
    "        self.packet_size_bins = tuple(packet_size_bins)\n",
    "        self.source_ports = tuple(source_ports)\n",
    "        self.destination_ports = tuple(destination_ports)\n",
    "        self.generic = generic\n",
    "        if not self.source_ports:\n",
    "            self.source_ports = ((0, 65535),)\n",
    "        if not self.destination_ports:\n",
    "            self.destination_ports = ((0, 65535),)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "    def __hash__(self) -> int:\n",
    "        return hash(str(self))\n",
    "\n",
    "    @staticmethod\n",
    "    def _port_check(port, defined_ports):\n",
    "        for defined_port in defined_ports:\n",
    "            if isinstance(defined_port, int):\n",
    "                if port == defined_port:\n",
    "                    return True\n",
    "            elif defined_port[0] <= port <= defined_port[1]:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def __contains__(self, flow: Flow) -> bool:\n",
    "        return self._port_check(\n",
    "            flow.source_port, self.source_ports\n",
    "        ) and self._port_check(flow.destination_port, self.destination_ports)\n",
    "\n",
    "    def __getitem__(self, value: T.Union[int, Flow]) -> T.Optional[str]:\n",
    "        if isinstance(value, Flow):\n",
    "            value = round(value.bytes / value.packets)\n",
    "        elif not isinstance(value, int):\n",
    "            raise ValueError(\n",
    "                \"Invalid interval calculation call. Must be a value or a flow.\"\n",
    "            )\n",
    "\n",
    "        for bin_range in self.packet_size_bins:\n",
    "            if bin_range[0] is None:\n",
    "                if bin_range[1] is not None and value <= bin_range[1]:\n",
    "                    return f\"<= {bin_range[1]}\"\n",
    "            elif bin_range[1] is None:\n",
    "                if bin_range[0] is not None and bin_range[0] <= value:\n",
    "                    return f\">= {bin_range[0]}\"\n",
    "            elif bin_range[0] <= value <= bin_range[1]:\n",
    "                return f\"{bin_range[0]}<->{bin_range[1]}\"\n",
    "\n",
    "        if self.packet_size_interval < 0:\n",
    "            return None\n",
    "        if self.packet_size_interval == 0:\n",
    "            return \"-\"\n",
    "\n",
    "        # Fall back to the interval binning approach if available.\n",
    "        interval = value // self.packet_size_interval\n",
    "        lower_interval = interval * self.packet_size_interval\n",
    "        upper_interval = (interval + 1) * self.packet_size_interval - 1\n",
    "        return f\"{lower_interval}<->{upper_interval}\"\n",
    "\n",
    "\n",
    "# The rules in this are based off our paper. You can add more if you want.\n",
    "# Correctness is extremely unlikely and not necessary.\n",
    "def UDP_DDoS_services(\n",
    "    generic_interval: int = PACKET_SIZE_INTERVAL,\n",
    "    domain_knowledge: bool = True,\n",
    ") -> T.Tuple[Service, ...]:\n",
    "    domain_knowledge_services = (\n",
    "        Service(\n",
    "            \"DNS DrDoS (UDP)\",\n",
    "            source_ports=(53, 853, 5353),\n",
    "            packet_size_bins=((551, None),),\n",
    "            packet_size_interval=-1,\n",
    "        ),\n",
    "        Service(\n",
    "            \"DNS (UDP)\",\n",
    "            source_ports=(53, 853, 5353),\n",
    "            packet_size_interval=100,\n",
    "        ),\n",
    "        Service(\n",
    "            \"Memcached DrDoS (UDP)\",\n",
    "            source_ports=(11211,),\n",
    "            packet_size_bins=((256, None),),\n",
    "        ),\n",
    "        Service(\n",
    "            \"SNMP DrDoS (UDP)\",\n",
    "            source_ports=(161,),\n",
    "            packet_size_bins=((151, None),),\n",
    "            packet_size_interval=-1,\n",
    "        ),\n",
    "        Service(\n",
    "            \"NTP DrDoS (UDP)\",\n",
    "            source_ports=(123,),\n",
    "            packet_size_bins=((100, None),),\n",
    "            packet_size_interval=-1,\n",
    "        ),\n",
    "        Service(\n",
    "            \"NTP (UDP)\",\n",
    "            source_ports=(123,),\n",
    "        ),\n",
    "        Service(\n",
    "            \"CLDAP DrDoS (UDP)\",\n",
    "            source_ports=(389,),\n",
    "            packet_size_bins=((None, 150), (151, None)),\n",
    "        ),\n",
    "        Service(\n",
    "            \"MSSQL (UDP)\",\n",
    "            source_ports=(1434,),\n",
    "            packet_size_interval=300,\n",
    "        ),\n",
    "        Service(\n",
    "            \"SSDP (UDP)\",\n",
    "            source_ports=(1900,),\n",
    "            packet_size_bins=((300, None),),\n",
    "            packet_size_interval=100,\n",
    "        ),\n",
    "        Service(\n",
    "            \"NetBIOS (UDP)\",\n",
    "            source_ports=(137, 138, 139),\n",
    "            packet_size_interval=100,\n",
    "        ),\n",
    "        Service(\n",
    "            \"CharGEN (UDP)\",\n",
    "            source_ports=(19,),\n",
    "        ),\n",
    "        Service(\n",
    "            \"TFTP (UDP)\",\n",
    "            source_ports=(69,),\n",
    "        ),\n",
    "        Service(\n",
    "            \"Kerberos DrDoS (UDP)\",\n",
    "            destination_ports=(88,),\n",
    "            packet_size_interval=32,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    generic_services = (\n",
    "        Service(\n",
    "            \"Generic (System Ports -> System Ports) (UDP)\",\n",
    "            source_ports=((0, 1023),),\n",
    "            destination_ports=((0, 1023),),\n",
    "            packet_size_interval=generic_interval,\n",
    "            generic=True,\n",
    "        ),\n",
    "        Service(\n",
    "            \"Generic (System Ports -> Registered Ports) (UDP)\",\n",
    "            source_ports=((0, 1023),),\n",
    "            destination_ports=((1024, 49151),),\n",
    "            packet_size_interval=generic_interval,\n",
    "            generic=True,\n",
    "        ),\n",
    "        Service(\n",
    "            \"Generic (System Ports -> Ephemeral Ports) (UDP)\",\n",
    "            source_ports=((0, 1023),),\n",
    "            destination_ports=((49152, 65535),),\n",
    "            packet_size_interval=generic_interval,\n",
    "            generic=True,\n",
    "        ),\n",
    "        Service(\n",
    "            \"Generic (Registered Ports -> System Ports) (UDP)\",\n",
    "            source_ports=((1024, 49151),),\n",
    "            destination_ports=((0, 1023),),\n",
    "            packet_size_interval=generic_interval,\n",
    "            generic=True,\n",
    "        ),\n",
    "        Service(\n",
    "            \"Generic (Registered Ports -> Registered Ports) (UDP)\",\n",
    "            source_ports=((1024, 49151),),\n",
    "            destination_ports=((1024, 49151),),\n",
    "            packet_size_interval=generic_interval,\n",
    "            generic=True,\n",
    "        ),\n",
    "        Service(\n",
    "            \"Generic (Registered Ports -> Ephemeral Ports) (UDP)\",\n",
    "            source_ports=((1024, 49151),),\n",
    "            destination_ports=((49152, 65535),),\n",
    "            packet_size_interval=generic_interval,\n",
    "            generic=True,\n",
    "        ),\n",
    "        Service(\n",
    "            \"Generic (Ephemeral Ports -> System Ports) (UDP)\",\n",
    "            source_ports=((49152, 65535),),\n",
    "            destination_ports=((0, 1023),),\n",
    "            packet_size_interval=generic_interval,\n",
    "            generic=True,\n",
    "        ),\n",
    "        Service(\n",
    "            \"Generic (Ephemeral Ports -> Registered Ports) (UDP)\",\n",
    "            source_ports=((49152, 65535),),\n",
    "            destination_ports=((1024, 49151),),\n",
    "            packet_size_interval=generic_interval,\n",
    "            generic=True,\n",
    "        ),\n",
    "        Service(\n",
    "            \"Generic (Ephemeral Ports -> Ephemeral Ports) (UDP)\",\n",
    "            source_ports=((49152, 65535),),\n",
    "            destination_ports=((49152, 65535),),\n",
    "            packet_size_interval=generic_interval,\n",
    "            generic=True,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        domain_knowledge_services + generic_services\n",
    "        if domain_knowledge\n",
    "        else generic_services\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics Generation\n",
    "Only used for some plots. Likely not interesting for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "Collection = T.Tuple[\n",
    "    T.Tuple[dt.datetime, dt.datetime],\n",
    "    int,  # Bytes.\n",
    "    int,  # Packets.\n",
    "    T.Dict[int, int],  # Flows using a source port.\n",
    "    T.Dict[int, int],  # Flows using a destination port.\n",
    "    T.Dict[T.Tuple[int, int], int],  # Flows using a port pair.\n",
    "]\n",
    "\n",
    "\n",
    "def statistics_collection(\n",
    "    datetime_interval: T.Tuple[dt.datetime, dt.datetime]\n",
    ") -> Collection:\n",
    "    byte_count = 0\n",
    "    packet_count = 0\n",
    "    source_ports: T.DefaultDict[int, int] = cll.defaultdict(int)\n",
    "    destination_ports: T.DefaultDict[int, int] = cll.defaultdict(int)\n",
    "    port_pairs: T.DefaultDict[T.Tuple[int, int], int] = cll.defaultdict(int)\n",
    "    for flow in flow_reader(*datetime_interval):\n",
    "        byte_count += flow.bytes\n",
    "        packet_count += flow.packets\n",
    "        source_ports[flow.source_port] += 1\n",
    "        destination_ports[flow.destination_port] += 1\n",
    "        port_pairs[(flow.source_port, flow.destination_port)] += 1\n",
    "    return (\n",
    "        datetime_interval,\n",
    "        byte_count,\n",
    "        packet_count,\n",
    "        dict(source_ports),\n",
    "        dict(destination_ports),\n",
    "        dict(port_pairs),\n",
    "    )\n",
    "\n",
    "\n",
    "def flow_dataset_statistics(\n",
    "    start: dt.datetime,\n",
    "    end: dt.datetime,\n",
    "    interval: dt.timedelta = dt.timedelta(weeks=1.0),\n",
    "):\n",
    "    with mp.Pool(os.cpu_count() or 8) as pool:\n",
    "        return pool.map(\n",
    "            statistics_collection, datetime_intervals(start, end, interval)\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Flow Corpus Generation (LSA, Doc2Vec, anything that uses documents, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "Corpus = T.Tuple[  # `DefaultDict` has trouble pickling here.\n",
    "    T.Dict[str, T.List[str]],\n",
    "    T.Dict[str, T.Dict[T.Optional[str], int]],\n",
    "]\n",
    "\n",
    "\n",
    "def flow_corpus_generation(\n",
    "    datetime_interval: T.Tuple[dt.datetime, dt.datetime],\n",
    "    services: T.Tuple[Service, ...],\n",
    ") -> Corpus:\n",
    "    documents: T.Dict[str, T.List[str]] = {}\n",
    "    labels: T.Dict[str, T.Dict[T.Optional[str], int]] = {}\n",
    "\n",
    "    for flow in flow_reader(*datetime_interval):\n",
    "        if flow.protocol != 17 or flow.source_address.version != 4:\n",
    "            continue\n",
    "        assert flow.destination_address.version == 4\n",
    "\n",
    "        tag = str(flow.destination_address)\n",
    "        words = documents.setdefault(tag, [])\n",
    "\n",
    "        for service in services:\n",
    "            if flow not in service:\n",
    "                continue\n",
    "\n",
    "            interval = service[flow]\n",
    "            if not interval:\n",
    "                continue\n",
    "\n",
    "            name_word = str(service)\n",
    "            interval_word = str(interval)\n",
    "            source_port_word = f\"{flow.source_port}->\"\n",
    "            destination_port_word = f\"->{flow.destination_port}\"\n",
    "            for _ in range(flow.packets):\n",
    "                words.append(name_word)\n",
    "                words.append(interval_word)\n",
    "                words.append(source_port_word)\n",
    "                words.append(destination_port_word)\n",
    "\n",
    "            break\n",
    "        else:\n",
    "            log(f\"Flow matched no services: {flow}\")\n",
    "\n",
    "        label = f\"{flow.rule}:{flow.rule_confidence}\" if flow.rule else None\n",
    "        if tag not in labels:\n",
    "            labels[tag] = {label: flow.bytes}\n",
    "        else:\n",
    "            tag_labels = labels[tag]\n",
    "            if label not in tag_labels:\n",
    "                tag_labels[label] = flow.bytes\n",
    "            else:\n",
    "                tag_labels[label] += flow.bytes\n",
    "\n",
    "    return (documents, labels)\n",
    "\n",
    "\n",
    "def flows_to_corpus(\n",
    "    start: dt.datetime, end: dt.datetime, domain_knowledge: bool\n",
    ") -> Corpus:\n",
    "    datetime_splits = tuple(\n",
    "        datetime_intervals(start, end, dt.timedelta(minutes=30))\n",
    "    )\n",
    "    services = UDP_DDoS_services(domain_knowledge=domain_knowledge)\n",
    "    with mp.Pool(os.cpu_count() or 8) as pool:\n",
    "        corpora: T.Deque[Corpus] = cll.deque(  # Already sorted.\n",
    "            pool.starmap(\n",
    "                flow_corpus_generation,\n",
    "                ((split, services) for split in datetime_splits),\n",
    "                chunksize=1,\n",
    "            ),\n",
    "            maxlen=len(datetime_splits),\n",
    "        )\n",
    "\n",
    "    final_documents: T.Dict[str, T.List[str]] = {}\n",
    "    final_labels: T.Dict[str, T.Dict[T.Optional[str], int]] = {}\n",
    "\n",
    "    log(f\"Finished corpora collection. Merging {len(corpora)} corpora...\")\n",
    "    while corpora:\n",
    "        documents, labels = corpora.popleft()\n",
    "\n",
    "        while documents:\n",
    "            tag, words = documents.popitem()\n",
    "            if tag in final_documents:\n",
    "                final_documents[tag].extend(words)\n",
    "            else:\n",
    "                final_documents[tag] = words\n",
    "\n",
    "        while labels:\n",
    "            tag, label_counts = labels.popitem()\n",
    "            if tag not in final_labels:\n",
    "                final_labels[tag] = {}\n",
    "            tag_labels = final_labels[tag]\n",
    "\n",
    "            while label_counts:\n",
    "                label, count = label_counts.popitem()\n",
    "                if label in tag_labels:\n",
    "                    tag_labels[label] += count\n",
    "                else:\n",
    "                    tag_labels[label] = count\n",
    "\n",
    "    return (final_documents, final_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Flow Corpus Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "Word2VecCorpus = T.Tuple[  # `DefaultDict` has trouble pickling here.\n",
    "    T.Dict[str, T.List[str]],\n",
    "    T.Dict[str, T.Dict[T.Optional[str], int]],\n",
    "]\n",
    "\n",
    "\n",
    "def flow_corpus_generation_word2vec(\n",
    "    datetime_interval: T.Tuple[dt.datetime, dt.datetime],\n",
    "    services: T.Tuple[Service, ...],\n",
    ") -> Word2VecCorpus:\n",
    "    sentences: T.DefaultDict[str, T.List[str]] = cll.defaultdict(list)\n",
    "    labels: T.Dict[str, T.Dict[T.Optional[str], int]] = {}\n",
    "\n",
    "    for flow in flow_reader(*datetime_interval):\n",
    "        if flow.protocol != 17 or flow.source_address.version != 4:\n",
    "            continue\n",
    "        assert flow.destination_address.version == 4\n",
    "\n",
    "        word = str(flow.destination_address)\n",
    "        sentences[f\"{flow.source_port}->\"].append(word)\n",
    "        sentences[f\"->{flow.destination_port}\"].append(word)\n",
    "\n",
    "        for service in services:\n",
    "            if flow not in service:\n",
    "                continue\n",
    "\n",
    "            interval = service[flow]\n",
    "            if not interval:\n",
    "                continue\n",
    "\n",
    "            sentences[str(service)].append(word)\n",
    "            sentences[interval].append(word)\n",
    "            break\n",
    "        else:\n",
    "            log(f\"Flow matched no services: {flow}\")\n",
    "\n",
    "        label = f\"{flow.rule}:{flow.rule_confidence}\" if flow.rule else None\n",
    "        if word not in labels:\n",
    "            labels[word] = {label: flow.bytes}\n",
    "        else:\n",
    "            tag_labels = labels[word]\n",
    "            if label not in tag_labels:\n",
    "                tag_labels[label] = flow.bytes\n",
    "            else:\n",
    "                tag_labels[label] += flow.bytes\n",
    "\n",
    "    return (dict(sentences), labels)\n",
    "\n",
    "\n",
    "def flows_to_word2vec_corpus(\n",
    "    start: dt.datetime, end: dt.datetime, domain_knowledge: bool\n",
    ") -> Word2VecCorpus:\n",
    "    datetime_splits = tuple(\n",
    "        datetime_intervals(start, end, dt.timedelta(minutes=30))\n",
    "    )\n",
    "    services = UDP_DDoS_services(domain_knowledge=domain_knowledge)\n",
    "    with mp.Pool(os.cpu_count() or 8) as pool:\n",
    "        corpora: T.Deque[Word2VecCorpus] = cll.deque(  # Already sorted.\n",
    "            pool.starmap(\n",
    "                flow_corpus_generation_word2vec,\n",
    "                ((split, services) for split in datetime_splits),\n",
    "                chunksize=1,\n",
    "            ),\n",
    "            maxlen=len(datetime_splits),\n",
    "        )\n",
    "\n",
    "    final_documents: T.Dict[str, T.List[str]] = {}\n",
    "    final_labels: T.Dict[str, T.Dict[T.Optional[str], int]] = {}\n",
    "\n",
    "    log(f\"Finished corpora collection. Merging {len(corpora)} corpora...\")\n",
    "    while corpora:\n",
    "        documents, labels = corpora.popleft()\n",
    "\n",
    "        while documents:\n",
    "            tag, words = documents.popitem()\n",
    "            if tag in final_documents:\n",
    "                final_documents[tag].extend(words)\n",
    "            else:\n",
    "                final_documents[tag] = words\n",
    "\n",
    "        while labels:\n",
    "            tag, label_counts = labels.popitem()\n",
    "            if tag not in final_labels:\n",
    "                final_labels[tag] = {}\n",
    "            tag_labels = final_labels[tag]\n",
    "\n",
    "            while label_counts:\n",
    "                label, count = label_counts.popitem()\n",
    "                if label in tag_labels:\n",
    "                    tag_labels[label] += count\n",
    "                else:\n",
    "                    tag_labels[label] = count\n",
    "\n",
    "    return (final_documents, final_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Counts Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "Counts = T.Tuple[\n",
    "    T.Dict[\n",
    "        str,\n",
    "        T.Dict[\n",
    "            str,\n",
    "            T.Tuple[\n",
    "                int,  # Flow count.\n",
    "                int,  # Byte count.\n",
    "                int,  # Packet count.\n",
    "            ],\n",
    "        ],\n",
    "    ],\n",
    "    T.Dict[str, T.Dict[T.Optional[str], int]],\n",
    "]\n",
    "\n",
    "\n",
    "def flow_counts(\n",
    "    datetime_interval: T.Tuple[dt.datetime, dt.datetime]\n",
    ") -> Counts:\n",
    "    counts: T.DefaultDict[\n",
    "        str, T.Dict[str, T.Tuple[int, int, int]]\n",
    "    ] = cll.defaultdict(dict)\n",
    "    labels: T.Dict[str, T.Dict[T.Optional[str], int]] = {}\n",
    "\n",
    "    for flow in flow_reader(*datetime_interval):\n",
    "        if flow.protocol != 17 or flow.source_address.version != 4:\n",
    "            continue\n",
    "        assert flow.destination_address.version == 4\n",
    "\n",
    "        tag = str(flow.destination_address)\n",
    "        tag_counts = counts[tag]\n",
    "\n",
    "        for port in (f\"{flow.source_port}->\", f\"->{flow.destination_port}\"):\n",
    "            if port in tag_counts:\n",
    "                tag_counts_port = tag_counts[port]\n",
    "                tag_counts[port] = (\n",
    "                    tag_counts_port[0] + 1,\n",
    "                    tag_counts_port[1] + flow.bytes,\n",
    "                    tag_counts_port[2] + flow.packets,\n",
    "                )\n",
    "            else:\n",
    "                tag_counts[port] = (1, flow.bytes, flow.packets)\n",
    "\n",
    "        label = f\"{flow.rule}:{flow.rule_confidence}\" if flow.rule else None\n",
    "        if tag not in labels:\n",
    "            labels[tag] = {label: flow.bytes}\n",
    "        else:\n",
    "            tag_labels = labels[tag]\n",
    "            if label not in tag_labels:\n",
    "                tag_labels[label] = flow.bytes\n",
    "            else:\n",
    "                tag_labels[label] += flow.bytes\n",
    "\n",
    "    return (dict(counts), labels)\n",
    "\n",
    "\n",
    "def flows_to_counts(start: dt.datetime, end: dt.datetime) -> Counts:\n",
    "    datetime_splits = tuple(\n",
    "        datetime_intervals(start, end, dt.timedelta(minutes=30))\n",
    "    )\n",
    "    with mp.Pool(os.cpu_count() or 8) as pool:\n",
    "        split_counts: T.Deque[Counts] = cll.deque(  # Already sorted.\n",
    "            pool.imap(flow_counts, datetime_splits),\n",
    "            maxlen=len(datetime_splits),\n",
    "        )\n",
    "\n",
    "    final_counts: T.Dict[str, T.Dict[str, T.Tuple[int, int, int]]] = {}\n",
    "    final_labels: T.Dict[str, T.Dict[T.Optional[str], int]] = {}\n",
    "\n",
    "    log(f\"Finished collection. Merging {len(split_counts)} split counts...\")\n",
    "    while split_counts:\n",
    "        counts, labels = split_counts.popleft()\n",
    "\n",
    "        while counts:\n",
    "            tag, tag_counts = counts.popitem()\n",
    "            if tag not in final_counts:\n",
    "                final_tag_counts = {}\n",
    "                final_counts[tag] = final_tag_counts\n",
    "            else:\n",
    "                final_tag_counts = final_counts[tag]\n",
    "\n",
    "            while tag_counts:\n",
    "                service, new_counts = tag_counts.popitem()\n",
    "                if service in final_tag_counts:\n",
    "                    old_counts = final_tag_counts[service]\n",
    "                    final_tag_counts[service] = (\n",
    "                        old_counts[0] + new_counts[0],\n",
    "                        old_counts[1] + new_counts[1],\n",
    "                        old_counts[2] + new_counts[2],\n",
    "                    )\n",
    "                else:\n",
    "                    final_tag_counts[service] = new_counts\n",
    "\n",
    "        while labels:\n",
    "            tag, label_counts = labels.popitem()\n",
    "            if tag not in final_labels:\n",
    "                final_labels[tag] = {}\n",
    "            tag_labels = final_labels[tag]\n",
    "\n",
    "            while label_counts:\n",
    "                label, count = label_counts.popitem()\n",
    "                if label in tag_labels:\n",
    "                    tag_labels[label] += count\n",
    "                else:\n",
    "                    tag_labels[label] = count\n",
    "\n",
    "    return (final_counts, final_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow Corpus Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "def create_dataset(\n",
    "    mode: T.Literal[\"Standard\", \"Word2Vec\", \"Baseline\"],\n",
    "    start: dt.datetime,\n",
    "    end: dt.datetime,\n",
    "    name: str,\n",
    "    domain_knowledge: bool = True,\n",
    "):\n",
    "    os.makedirs(ARTEFACTS_PATH, mode=0o770, exist_ok=True)\n",
    "\n",
    "    log(f'Generating training data \"{name}\"...')\n",
    "    if os.path.isfile(os.path.join(ARTEFACTS_PATH, f\"{name}.data.pickle.gz\")):\n",
    "        log(f'Training data \"{name}\" already exists.')\n",
    "        return\n",
    "\n",
    "    if mode == \"Standard\":\n",
    "        data, labels = flows_to_corpus(\n",
    "            start, end, domain_knowledge=domain_knowledge\n",
    "        )\n",
    "    elif mode == \"Word2Vec\":\n",
    "        data, labels = flows_to_word2vec_corpus(\n",
    "            start, end, domain_knowledge=domain_knowledge\n",
    "        )\n",
    "    elif mode == \"Baseline\":\n",
    "        data, labels = flows_to_counts(start, end)\n",
    "    save_pickle(data, os.path.join(ARTEFACTS_PATH, f\"{name}.data.pickle.gz\"))\n",
    "    save_pickle(\n",
    "        labels, os.path.join(ARTEFACTS_PATH, f\"{name}.labels.pickle.gz\")\n",
    "    )\n",
    "    log(f'Generated training data \"{name}\".')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell will create the datasets necessary for the baseline comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "for start, end, prefix in COMPARISON_TRAINING_DATES:\n",
    "    create_dataset(\"Standard\", start, end, f\"{prefix}.standard\")\n",
    "    create_dataset(\"Word2Vec\", start, end, f\"{prefix}.word2vec\")\n",
    "    create_dataset(\"Baseline\", start, end, f\"{prefix}.baseline\")\n",
    "    # No Domain Knowledge (NDK) versions.\n",
    "    create_dataset(\n",
    "        \"Standard\",\n",
    "        start,\n",
    "        end,\n",
    "        f\"{prefix}.standard-ndk\",\n",
    "        domain_knowledge=False,\n",
    "    )\n",
    "    create_dataset(\n",
    "        \"Word2Vec\",\n",
    "        start,\n",
    "        end,\n",
    "        f\"{prefix}.word2vec-ndk\",\n",
    "        domain_knowledge=False,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell will create the datasets necessary for the longitudinal analysis of a DDoS2Vec model's performance.\n",
    "\n",
    "**Note**: Our experiment covered all of 2019, but your dataset might not cover that range in monthly intervals. Modify accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "for month_start, month_end in zip(range(1, 12), range(2, 13)):\n",
    "    create_dataset(\n",
    "        \"Standard\",\n",
    "        dt.datetime(2019, month_start, 1, tzinfo=dt.timezone.utc),\n",
    "        dt.datetime(2019, month_end, 1, tzinfo=dt.timezone.utc),\n",
    "        f\"{month_start}.standard\",\n",
    "    )\n",
    "\n",
    "create_dataset(\n",
    "    \"Standard\",\n",
    "    dt.datetime(2019, 12, 1, tzinfo=dt.timezone.utc),\n",
    "    dt.datetime(2020, 1, 1, tzinfo=dt.timezone.utc),\n",
    "    \"12.standard\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics Selection\n",
    "\n",
    "Plotting code specific to our private dataset has been stripped, but we keep the code for generating the statistics.\n",
    "\n",
    "**Note**: Again, you must modify the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "statistics_file_path = os.path.join(ARTEFACTS_PATH, \"statistics.pickle.gz\")\n",
    "if os.path.isfile(statistics_file_path):\n",
    "    log(\"Statistics already exist. Loading...\")\n",
    "    statistics: T.List[Collection] = load_pickle(statistics_file_path)\n",
    "else:\n",
    "    log(\"Collecting statistics (weekly)...\")\n",
    "    statistics = flow_dataset_statistics(\n",
    "        dt.datetime(2019, 1, 1, tzinfo=dt.timezone.utc),\n",
    "        dt.datetime(2020, 1, 1, tzinfo=dt.timezone.utc),\n",
    "    )\n",
    "    save_pickle(statistics, statistics_file_path)\n",
    "    log(\"Collected statistics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "weekly_flow_counts: T.List[int] = [\n",
    "    sum(collection[3].values()) for collection in statistics\n",
    "]\n",
    "\n",
    "print(f\"Total UDP flow samples in dataset: {sum(weekly_flow_counts):,}\")\n",
    "\n",
    "average_daily_flow_counts = [\n",
    "    sum(collection[3].values()) / 7 for collection in statistics\n",
    "]\n",
    "average_daily_flow_count = round(\n",
    "    sum(average_daily_flow_counts) / len(average_daily_flow_counts)\n",
    ")\n",
    "\n",
    "print(f\"Daily mean UDP flow sample count: {average_daily_flow_count:,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
